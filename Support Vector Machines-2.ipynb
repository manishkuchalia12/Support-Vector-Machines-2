{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb90fd5-964e-4c5b-8442-043470f249b4",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e68211-0e93-4a45-af50-3b262b514632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features (important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Polynomial kernel with degree 2\n",
    "poly_svm = SVC(kernel='poly', degree=2, C=1)\n",
    "poly_svm.fit(X_train, y_train)\n",
    "\n",
    "# Plot decision boundary and support vectors\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for polynomial kernel\n",
    "plot_decision_boundary(poly_svm, X_train, y_train, 'Polynomial Kernel (Degree 2)')\n",
    "\n",
    "# Polynomial function for visualization\n",
    "def polynomial_function(x):\n",
    "    return x[:, 0]**2 + x[:, 1]**2 + 2 * x[:, 0] * x[:, 1]\n",
    "\n",
    "# Visualize polynomial function\n",
    "x_visualization = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "z_visualization = polynomial_function(np.c_[x_visualization[0].ravel(), x_visualization[1].ravel()])\n",
    "z_visualization = z_visualization.reshape(x_visualization[0].shape)\n",
    "\n",
    "plt.contourf(x_visualization[0], x_visualization[1], z_visualization, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Paired, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Polynomial Function Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43b45b-3d02-4527-861a-e8f278c4efb1",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a10e7e-f603-4b74-9357-b772af8bb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features (important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# SVM with polynomial kernel\n",
    "poly_svm = SVC(kernel='poly', degree=2, C=1)\n",
    "poly_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = poly_svm.predict(X_test)\n",
    "\n",
    "# Accuracy on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot decision boundary and support vectors\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('SVM with Polynomial Kernel')\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for polynomial kernel\n",
    "plot_decision_boundary(poly_svm, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695fba5-e0df-43c7-b004-20b93b6adadd",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "Ans:-\r\n",
    "In Support Vector Regression (SVR), the parameter epsilon (ε) is associated with the width of the epsilon-insensitive tube. This tube determines the range within which errors are not penalized in the loss function. Specifically, data points within the tube are considered sufficiently close to the predicted values and do not contribute to the loss.\r\n",
    "\r\n",
    "Increasing the value of epsilon generally widens the epsilon-insensitive tube. A wider tube allows for larger deviations between the predicted and actual values without incurring a penalty. As a result, more data points fall within the tube and are treated as support vectors.\r\n",
    "\r\n",
    "Here's an example in Python using Scikit-learn to demonstrate how changing the epsilon value affects the number of support vectors in SVR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf900f0-9e4e-4f71-a0f5-85ee5071b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y = np.sin(X).ravel() + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "# Function to fit SVR with different epsilon values\n",
    "def fit_and_plot_svr(epsilon):\n",
    "    svr = SVR(kernel='linear', epsilon=epsilon)\n",
    "    svr.fit(X_train, y_train)\n",
    "\n",
    "    plt.scatter(X_train, y_train, color='black', label='Data')\n",
    "    plt.plot(X_test, svr.predict(X_test), color='red', label='SVR Prediction')\n",
    "    plt.scatter(X_train[svr.support_], y_train[svr.support_], facecolors='none', edgecolors='blue', label='Support Vectors')\n",
    "    plt.title(f'SVR with Epsilon = {epsilon}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Number of Support Vectors: {len(svr.support_)}\")\n",
    "\n",
    "# Fit and plot SVR with different epsilon values\n",
    "epsilon_values = [0.1, 0.5, 1.0]\n",
    "for epsilon in epsilon_values:\n",
    "    fit_and_plot_svr(epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a68b9e-5ae4-45b4-a34c-395d9f46f022",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "Ans:-Support Vector Regression (SVR) is sensitive to the choice of several parameters, and understanding their impact is crucial for obtaining good performance. The key parameters in SVR are the choice of the kernel function, the regularization parameter (C), the epsilon parameter (ε), and the kernel-specific parameter (gamma). Let's discuss each parameter and its impact:\r\n",
    "\r\n",
    "1. Kernel Function:\r\n",
    "Role: The kernel function determines the type of mapping used to transform the input features into a higher-dimensional space.\r\n",
    "Impact: Different kernels capture different types of relationships in the data. Common kernels include linear, polynomial, and radial basis function (RBF or Gaussian).\r\n",
    "Example:\r\n",
    "Use a linear kernel for linear relationships.\r\n",
    "Use a polynomial kernel when the relationship is polynomial.\r\n",
    "Use an RBF kernel for complex, non-linear relationships.\r\n",
    "2. Regularization Parameter (C):\r\n",
    "Role: The C parameter controls the trade-off between fitting the training data well and having a smooth decision function. A smaller C encourages a simpler model, while a larger C allows the model to fit the training data more closely.\r\n",
    "Impact: A smaller C may lead to underfitting, and a larger C may lead to overfitting.\r\n",
    "Example:\r\n",
    "Use a smaller C when you want a smoother model or when dealing with noisy data.\r\n",
    "Use a larger C when you want the model to closely fit the training data.\r\n",
    "3. Epsilon Parameter (ε):\r\n",
    "Role: The epsilon parameter determines the width of the epsilon-insensitive tube. It defines the range within which errors are not penalized in the loss function.\r\n",
    "Impact: A smaller epsilon tightens the tube, making the model more sensitive to errors. A larger epsilon allows for larger errors without penalty.\r\n",
    "Example:\r\n",
    "Use a smaller epsilon when you want the model to be less tolerant of errors.\r\n",
    "Use a larger epsilon when you want to allow larger deviations between predicted and actual values.\r\n",
    "4. Gamma Parameter (for RBF Kernel):\r\n",
    "Role: The gamma parameter defines the influence of a single training example, with low values meaning a large influence and high values meaning a small influence.\r\n",
    "Impact: A smaller gamma leads to a wider decision region, making the model generalize more. A larger gamma makes the decision region more focused on individual data points.\r\n",
    "Example:\r\n",
    "Use a smaller gamma for smoother decision boundaries and when dealing with a large number of samples.\r\n",
    "Use a larger gamma when the data is more clustered, and you want the model to focus on individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f766a-11ef-4127-b6b3-868de5255c52",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
